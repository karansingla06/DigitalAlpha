"""
@author: karan
"""

# =============================================================================
# import tika
# from tika import parser
# parsed = parser.from_file(r'C:\Users\user\Downloads\Anthem1.docx', xmlContent=True)
# =============================================================================

import mammoth
with open(r"C:\Users\karan\Downloads\Anthem_Bronze_Pathway.docx", "rb") as docx_file:
    result = mammoth.convert_to_html(docx_file)
    html = result.value # The generated HTML
    messages = result.messages # Any messages, such as warnings during conversion
f = open(r"C:\Users\karan\Downloads\Anthem_Bronze_Pathway.html","w")
f.write(html)

from bs4 import BeautifulSoup
soup = BeautifulSoup(html,'html.parser')
#soup.prettify()

blacklist = ["img" ]
for tag in soup.findAll():
        if tag.name.lower() in blacklist:
            # blacklisted tags are removed in their entirety
            tag.extract()



import nltk
from nltk.tokenize import sent_tokenize,word_tokenize
from nltk.corpus import stopwords
from string import punctuation

from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english")


user_input=input("please enter your query:\n")

sents = sent_tokenize(user_input)
word_sent = word_tokenize(user_input.lower())

_stopwords = set(stopwords.words('english') + list(punctuation))
user_input_words=[word for word in word_sent if word not in _stopwords]
user_input_words=[stemmer.stem(i) for i in user_input_words]


# =============================================================================
# for item in word_sent[2:4]:
#         tokenized = nltk.word_tokenize(item)
#         tagged = nltk.pos_tag(tokenized)
#         print(tagged)
# 
# 
# =============================================================================

all_h1=soup.find_all('h2')

d_ms={}
for item in all_h1:
    first=item
    second=first.find_next('h2')
    temp=""
    
    while first.findNext()!=second:
        temp+= str(first.findNext().text)+" "
        first=first.findNext()
    d_ms[str(item.text)]=temp

for item,value in d_ms.items():
    print(item,"\n",value,'-----------------','\n')    
    


d_kwds={}
for key,val in d_ms.items():
    sents = sent_tokenize(val)
    word_sent = word_tokenize(val.lower())
    _stopwords = set(stopwords.words('english') + list(punctuation))
    section_words=[word for word in word_sent if word not in _stopwords]
    d_kwds[key]=set(section_words)
    
    
d_stem={}
for key,val in d_kwds.items():
    singles = [stemmer.stem(i) for i in val]
    d_stem[key]=set(singles)
    
    
key_section={}
for i in user_input_words:
    temp=[]
    for key,val in d_stem.items(): 
        if i in val:
            temp.append(key)
    key_section[i]=temp
        


##### finding heading in tables
tables = soup.find_all('table')
table1=tables[0]




